---
title: "Enhance Enterprise Document Using RAG with LLM"
subtitle: "Bertter search with Gen AI"
author: "Vijay Mohanam (Advisor: Jeremy Straub)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Emerging Trends in Artificial Intellegence
bibliography: genairag.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

The operational knowledge of modern enterprises is fragmented across heterogeneous systems, encompassing documents such as technical manuals, compliance guidelines, research reports, and HR repositories. This dispersion creates a fundamental bottleneck for context retrieval at scale, directly impairing the precision and utility of generative AI outputs. Traditional search solutions, based on keyword matching and intricate indexing architectures, are poorly suited to this task. Their reliance on statistical relevancy metrics often yields superficial results—typically brief, decontextualized text extracts accompanied by document links—rather than synthesizing the coherent, topic-specific knowledge necessary for advanced analytical or generative tasks.

The field of Environmental Health and Safety (EHS) comprises numerous divisions, including the Occupational Safety and Health Administration (OSHA), which sets standards for physical, chemical, biological, and radiological hazards. Ensuring compliance with these standards, such as those for "Fall Prevention," presents a twofold challenge: employers must supply appropriate equipment, and workers must accurately understand and apply the corresponding guidelines. This knowledge dissemination is severely hindered in many field settings—particularly in industries like construction—where workers may have low digital literacy and lack access to networked computers or stable power. To bridge this gap, this study investigates the feasibility of a localized LLM-RAG (Retrieval-Augmented Generation) system. Such a tool would enable workers to interact with dense regulatory documents using natural language queries, facilitating rapid, on-demand access to summarized guidelines or specific answers, thereby circumventing traditional barriers of document retrieval and navigation.
<!-- -->



## Background

LLM : Large Language Models (LLMs) are machine learning models trained on large volumes of text data to perform natural language understanding and generation tasks. These models are built on architectures like transformers, which utilize attention mechanisms to focus on different parts of the input text for context-aware processing and help in translation, summarization, question answering etc. There are 10's of models either open source or with subscription.

LLMs inherit a critical limitation: their knowledge is frozen at the point of training. Consequently, they struggle with temporal dynamism (e.g., recent events) and often lack proficiency in highly specialized or niche domains not well-represented in their pre-training data.

RAG : Retrieval-Augmented Generation (RAG) is a method designed to enhance the capabilities of traditional large language models (LLMs) by integrating them with external information retrieval systems. a RAG with local vector database of new/updated documents from a vast corpus of organization data.

There are few initiatives/studies/implementations from agricultural[@cottonbot] to common household controls[@monitor] using LLM, RAG and agentic. 

In 2023, the use of LLMs saw significant growth in organisation applications, particularly in the domain of RAG & information retrieval. an impressive 36.2% of organisation LLM use cases now employ RAG technology. RAG brings the power of LLMs to structured and unstructured data, making organisation information retrieval more effective and efficient than ever.

## System Summary

A RAG pipeline consists of three key components: retrieval, augmentation, and generation, each playing an essential role in generating accurate, context-aware outputs.

- **Retrieval:** The retrieval step is where the system searches an external knowledge base to gather relevant information, can include documents, articles, or web resources. 

- **Augmentation:** Once the relevant data is retrieved, the augmentation step kicks in. Here, the retrieved information is used as additional context for the LLM, helping it generate a more accurate and relevant response.

- **Generation:** The final stage is generation, where the language model processes the augmented data and creates a coherent, context-aware response.

![RAG components](RAG.png) Figure 1: RAG system components[@ragsource]

As this design is to run in a local computer, among so many LLM Llama3.1:8b was selected, which has 8 billion parameters with size of ~5GB and can comfortably run in 24GB RAM, used "nomic-embed-text" embedding model, which is simple with no multi-lingual support. our programing language is Python 3.10 with terminal iterations for this study, it can be enhanced with FastAPI for serving with other applications, or use StreamLit for web interface to interact with model, finally the vector database will be Chroma, which can store our local documents.

![Llama components](llama.png){width=500px}<BR>Figure 2: Llama components

## Data and Analysis

- **Data Collection**
Among 100's of guidlines in "Fall preventions" category, which includes agriculture, crane, construction lift, electric towers, radio signal antenna, windmill etc, one primary tool is the ladder, collected about 7 documents from [osha.gov](https://www.osha.gov/publications/bytopic/fall-prevention-protection), though a program routine can download relavent documents, as this will be off-line system, downloaded and stored in docs directory.

![docs](docs.png){width=600px}<BR>Figure 3: Docs

- **Data Preparation**
LangChain's PyPDFLoader method is used to load pdf docs, content of document are split into chunks using TextSplitter method to make is suitable to be stored in vector database as high-dimensional vectors or embeddings.


## Results

Among the docs, we have details of many types of ladders, so, for question 'what is a stepladder', we get precise answer with citation to know whaere it was extracted.

![docs](res01.png){width=700px}<BR>Figure 4: Q='What is a stepladder?'

Now, with many types/capacity of ladder, user is interested to know the capacity of a heavy duty ladder, the answer was right to the expectation.

![docs](res02.png){width=700px}<BR>Figure 5: Q='what is the load capacity of heavy duty ladder?'

May be user was working in a metric locally, so what to know the capacity in kilograms, observe LLM was able to understand the context, and did conversion to KG, not found in the document.

![docs](res04.png){width=700px}<BR>Figure 6: Q='what is the load capacity of heavy duty ladder, report in kg'

For any more details or to report to OSHA, user want to know option to contact OSHA office

![docs](res03.png){width=700px}<BR>Figure 7: Q='how to contact OSHA?'



```{r}

```

## Conclusion

This study covered key steps in building a RAG to be used without internet/intranet service at the worksite using small and powerful LLM, from loading and chunking pdf docs from osha.gov site to using embeddings and vector databases like Chroma. Retrieve relevant documents and generate meaningful responses.

## References
