---
title: "Lasso - Ridge Regression Group for data science projects - Fall 2025 "
subtitle: "A report on Lasso-Ridge Regression"
author: "Vijay Mohanam, Jennifer Nesbit and Sassou Khabou (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

::: callout-note
**Week 01:** This is our first week as group, we are trying to compile a summary of introductions!

**Week 02:** We tried to add more articles from our research!

Nice effort!
:::

## Introduction

Least Absolute Shrinkage and Selection Operator (Lasso) and Ridge regression are both regularization techniques used to prevent overfitting and multicollinearity in linear regression models by adding a penalty term to the loss function. The Lasso regression method was first introduced by Tibshirani in 1996 and has become a widely used method in both statistical modeling and machine learning. Lasso regression uses L1 regularization, which adds a penalty proportional to the sum of the absolute values of the coefficients, whereas Ridge uses L2 regularization, which adds a penalty proportional to the sum of the squared coefficients. An additional regularization technique called Elastic Net regression, combines both Lasso and Ridge. This technique is useful when there are many correlated features. The L1 penalty used with Lasso shrinks some coefficients to zero which allows for automated selection while also reducing model complexity. Overall Lasso is able to combine both predictive accuracy with interpretability. 

Lasso has demonstrated its versatile and practical uses as it has been applied across diverse applications. In healthcare, Lasso has been used to predict prevalence of child malaria Aheto et al., 2021[@aheto], stroke risk in hypertensive patients Huang & Liu, 2025[@huang] and adverse drug reactions in polypharmacy Sommer et al., 2024 [@sommer]. These studies all highlight the ability for Lasso to produce models with fewer predictors while maintaining strong performance and often outperforming Ridge regression models in interpretability. Lasso has also been applied to clinical diagnostics, specifically improving disease classification for glaucoma progression Fujino et al., 2015 [@fujino] and differentiating between hematologic cancers Amaador et al., 2022[@amaador]. Another medical related paper on gastroenterology (Ali, H et al., 2023)[@ali] also applied Lasso to select key predictors and remove insignificant predictors by setting their coefficients to zero. These applications demonstrate the ability of Lasso to enhance decision making with streamlined models.

Lasso regression has also been proven as an effective methodology in economics and finance. Specifically, in predicting corporate bankruptcy Pereira et al., 2016[@pereira] and analyzing housing price network connectedness Gabauer et al., 2024[@gabauer]. Lasso has also been used for technology applications such as supporting innovations specifically with sensor selection for wearable respiratory monitoring devices Laufer et al., 2023 [@laufer] and improved interpretability with decision tree models Czajkowski et al., 2023 [@czajkowski].

Research has been done to compare the use of Lasso as a methodology to multiple related techniques such as Ridge and Elastic Net. Studies have shown that Ridge regression sometimes outperforms Lasso when predictor variables are highly correlated. However, Lasso tends to be more valuable when only a few predictors are significant and applicable to the problem at hand Vidaurre et al., 2013[@vidaurre]. Overall, Lasso shows a consistent advantage with its ability to produce sparse, interpretable and efficient models making it an excellent tool for both modeling and research.

A principal advantage of Lasso (Least Absolute Shrinkage and Selection Operator) regression is its capacity for feature shrinkage and selection. As established in the seminal works by “Introduction to the Lasso” (Gauraha, N, 2018)[@gauraha], selection of most relevant variables from a large set of clinical and pathological data while shrinking less important ones to zero(Li, Y., Bai. 2023)[@li] and “Regression Shrinkage and Selection Via the Lasso” (Robert Tibshirani. 1996)[@robert]. They also compare lasso regression to both ridge regression and subset selection. Unlike in ridge regression, where all parameters can only asymptotically approach zero, lasso regression allows some parameters to reach zero. These techniques are applied in ML for feature selection techniques in predictive modeling (R. Muthukrishnan and R. Rohini. 2016)[@muthu] .On the same lines, we can see how to handle zero inflated predictors(Mariella Gregorich et al., 2025)[@mari] handled by Lasso.

*The final introduction paragraph on the problem that we will apply Lasso to will be added to draft next week after finalizing the problem and choosing a dataset.*




## Methods


## Analysis and Results


## Data Exploration and Visualization


## Modeling and Results


## Conclusion


## References

