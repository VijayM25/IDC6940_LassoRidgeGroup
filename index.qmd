---
title: "Lasso - Ridge Regression Group for data science projects - Fall 2025 "
subtitle: "A report on Lasso-Ridge Regression"
author: "Vijay Mohanam, Jennifer Nesbit and Sassou Khabou (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

::: callout-note
**Week 01:** This is our first week as group, we are trying to compile a summary of introductions!

Nice report!
:::

## Introduction

Lasso and Ridge regression are both regularization techniques used to prevent overfitting and multicollinearity in linear regression models by adding a penalty term to the loss function. Lasso regression uses L1 regularization, which adds a penalty proportional to the sum of the absolute values of the coefficients, whereas Ridge uses L2 regularization, which adds a penalty proportional to the sum of the squared coefficients. There is another one called Elastic Net regression, which combines both Lasso and Ridge, this works well when there are many correlated features.

We analysed the following papers in our initial understanding of how it is used in various industries.

1. https://doi.org/10.1167/iovs.15-16445
2. https://doi.org/10.3390/s23177407
3. https://doi.org/10.1080/10428194.2021.2018584
4. https://doi.org/10.1186/s12957-023-03097-4
5. https://doi.org/10.1097/MD.0000000000042690
6. https://doi.org/10.1016/j.ins.2023.118944
7. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8258678
8. https://onlinelibrary.wiley.com/doi/10.1002/sim.70062
9. https://www.ncbi.nlm.nih.gov/pmc/articles/39521861
10. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7678959
11. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7678959
12. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7678959
13. https://doi.org/10.1007/s12045-018-0635-x
14. https://doi.org/10.1111/j.2517-6161.1996.tb02080.x
15. https://doi.org/10.1111/insr.12023
16. Link NA
17. Link NA
18. https://doi.org/10.1007/s12664-023-01426-9


------------------ Updated till here -----------------------
<!--Team updates -->

-   Introduce why it is relevant needs.

-   Provide an overview of your approach.

Example of writing including citing references:

*This is an introduction to ..... regression, which is a non-parametric
estimator that estimates the conditional expectation of two variables
which is random. The goal of a kernel regression is to discover the
non-linear relationship between two random variables. To discover the
non-linear relationship, kernel estimator or kernel smoothing is the
main method to estimate the curve for non-parametric statistics. In
kernel estimator, weight function is known as kernel function
[@efr2008]. Cite this paper [@bro2014principal]. The GEE [@wang2014].
The PCA [@daffertshofer2004pca]*. Topology can be used in machine
learning [@adams2021topology]

For Symbolic Regression [@wang2019symbolic] *This is my work and I want
to add more work...*

Cite new paper [@su2012linear]

## Methods

-   Detail the models or algorithms used.

-   Justify your choices based on the problem and data.

*The common non-parametric regression model is*
$Y_i = m(X_i) + \varepsilon_i$*, where* $Y_i$ *can be defined as the sum
of the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is
unknown and* $\varepsilon_i$ *some errors. With the help of this
definition, we can create the estimation for local averaging i.e.*
$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$
*is near to* $x$*. In other words, this means that we are discovering
the line through the data points with the help of surrounding data
points. The estimation formula is printed below [@R-base]:*

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if* $X_i$ *is far from* $x$*.*

*Another equation:*

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```


```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References
