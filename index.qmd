---
title: "Writing a great story for data science projects - Fall 2025 "
subtitle: "This is a Report using Quarto"
author: "Vijay Mohanam (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

::: callout-note
**Individual research:**
<BR>[Week 1 Paper 1](#week-1-paper-1) 
<BR>[Week 1 Paper 2](#week-1-paper-2) 
:::

## Week 1 Paper 1

A predictive model, and predictors of under-five child malaria prevalence in Ghana:[@NIH]

##### How do 'Least Absolute Shrinkage and Selection Operator'(LASSO), Ridge and Elastic net regression approaches compare?

-   To understand how Lasso, Rdige and Elastic was used in prediction.

-   Particularly in features selections

-   How it affected the fitted model.

<!-- -->


*Its yet another paper on Malaria prediction in a community, this improves and focus on 
selections of features that best fit for Machine Learning models. Malaria among childhood in Sub-Saharan Africa is one of the leading cause of under-five mortality[@Maitlan], [@Camponova].

The interesting method of approach the author handles on elaborating LASSO regression for smallest number of predictors with the smallest prediction error. If this Machine Learning model is able to predict with smallest error, will help avoid adverse outcome after childhood malaria underscores the need for early detection and identification of high-risk populations.

The tricky part is Malaria can have wide range of covariates like physical, climatic and social factors, selecting minimal predictors which will have impact on outcome is where Lasso and Ridge regression will help.

## Data collection

The author has distributed data collection from rural and urban regions with the total of 2867 children under-five, using rapid diagnostic test(RDK) kit who got infected with malaria.

With the available dataset and literature search, they narrowed down to following variables


## Predictors

-   Child age, 
-   Number of under-five children in a household, 
-   Has mosquito bed net for sleeping, 
-   Sex of household head, 
-   Sex of a household member, 
-   Dwelling sprayed against mosquito last 12 months, 
-   Household wealth, 
-   Child-anaemia status, 
-   Has electricity in HH, 
-   Has a television in the household, 
-   Place of residence, 
-   The region of residence, 
-   Number of children who slept under mosquito bed net previous night, 
-   Insecticide-treated net available in the household, 
-   Number of household members.


## Alpha selection

For LASSO, author selected an alpha value of one, while for Ridge regression, an alpha value of zero was chosen. Since the alpha values for Elastic Net fall between zero and one 
(0<α<1), author used maximum likelihood estimation to determine the optimal alpha, which was estimated to be approximately 0.4187, based on 5-fold cross-validation repeated five times using the ‘caret’ package. Additionally, we estimated the minimum lambda (corresponding to the lowest mean squared error, or MSE) for LASSO, Ridge, and Elastic Net through maximum likelihood estimation under k-fold cross-validation.


### Modeling and Results

-   Lasso was able to predict using 11 features.

-   Ridge used all 15 features.

|Model |R-Square | RMSE(95% CI)|SD|AUC Value|Predictors|
|:------|:------ |:------------|:------|:------|:------|
|Lasso |0.196989 |0.9489 (0.9286, 0.9691) |0.0202 |81.20% |11 |
|Ridge |0.1972966 |1.0366 (1.0194, 1.0537) |0.0172 |81.20% |15 |

### Conclusion

-   Among two of logistic models using features picked by LASSO and Ridge methods. The model with LASSO features used 11 features and the Ridge model used 15 features. Both of these models explained about 20% of the differences in malaria rates among children. Each model had the same accuracy score for predictions, with an area under the curve (AUROC) of 81.2%. This means the models were quite good at predicting which children might have malaria.

-   Based on the principle of parsimony, the Lasso regression is preferred because it contains the smallest number of predictors and the smallest prediction error. LASSO model (RMSE = 0.9489, SD = 0.0202) where as Ridge (RMSE = 1.0366, SD = 0.0172) where RMSE is root mean square error i.e., prediction error

## Week 1 Paper 2

Prediction Modeling With Many Correlated and Zero-Inflated Predictors: Assessing the Non-negative Garrote Approach:[@wiley]

##### How do to handle zero-inflation data using 'ridge-garrote' methods?

-   To understand how to transform zero inflated predictor.

-   Particularly in correlated features

-   How it affected the fitted model.

<!-- -->


*This paper is of great inerest because it lists ways to handle zero values in the samples specifically in mass-spectrometry dataset.

The authors apply this technique and document there observations using predict kidney function using peptidomic features. The issue with mass-specrometry is that it produces intense data, sometime not all data is needed for accurate prediction another problem is that they are often subject to zero-inflation, means there is s spike at zero, which needs special transformation for linear models.


## Data collection

In Exploratory Data Analysis, Zero-inflated dataset is sometimes called as point mass values(PMVs, non-PMVs). There could be two reason, one natural sampleing data itself has zero or recording tool could be at fault. once eliminating the tool accuracy, we have to address actual PMVs.

Though there are methods of handling PVM, demonstrated by Gajjala et al.[@gajjala] used the least absolute shrinkage and selection operator (lasso), nonetheless, the optimal strategy for handling and analyzing zero-inflated and correlated predictors is uncertain, particularly in high-dimensional settings.


## Models
Author investigated commonly used practice, lasso and ridge regression, lasso provides selection and shrinkage of the regression coefficients for predictors based on L1-regularization where as ridge is better suited to handle correlated predictors.

In Lasso-ridge approach, author concludes, reduces the set of candidate predictors, while the second step retains the information from the zero-inflated predictors

In Ridge-lasso approach, author observes that there is a good blend of two-stage approach, which leads to explore ridge-garrote approach, first fits a ridge regression model then shrinkage factor for all predictors.

## Data selection

Motivated by real-life data from mass-spectrometry studies, author simulated data from a multivariate binomial and lognormal mixture distribution, 4 groups of 200 predictors with a group size of 50 predictors carefully introducing zeros in the dataset ranging 1/3 to 2/3 sample zeros.


### Modeling and Results


From each simulated dataset

-   Ridge, Lasso
-   Lasso-ridge
-   ridge-lasso
-   ridge-garrote regression were computed.

![Result](PVM.png){fig-alt="Result" width="400"}

Plot above displays the modeling methods across various cutoff levels for the maximum allowed proportion of PMVs (maxPMV) to explore how sensitive results are to these cutoff levels. Ridge, lasso-ridge and ridge-lasso mostly overlapped in terms of predictive accuracy in terms of RMSPE and R^2^
 

### Conclusion

-   Author concludes as ridge-garrote model was with the smallest number of selected predictors and demonstrated stability in predictor selection across increasing levels of maxPMV in contrast to lasso-ridge.
-   The lasso-ridge model, despite its predictive capabilities, showed similar variability in terms of the number of selected variables, can be seen by the sudden jump in selected predictors at a maximum percentage of zero-inflation of 90%.

## References
