---
title: "Writing a great story for data science projects - Fall 2025 "
subtitle: "This is a Report using Quarto"
author: "Vijay Mohanam (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

::: callout-note
**Individual research:**
<BR>[Week 1 Paper 1](#week-1-paper-1) 
<BR>[Week 1 Paper 2](#week-1-paper-2) 
<BR>[Week 2 paper 1](#week-2-paper-1) 
<BR>[Week 2 paper 2](#week-2-paper-2) 
<BR>[Week 3 paper 1](#week-3-paper-1) 
<BR>[Week 3 paper 2](#week-3-paper-2) 

:::

## Week 1 Paper 1

A predictive model, and predictors of under-five child malaria prevalence in Ghana:[@NIH]

##### How do 'Least Absolute Shrinkage and Selection Operator'(LASSO), Ridge and Elastic net regression approaches compare?

-   To understand how Lasso, Rdige and Elastic was used in prediction.

-   Particularly in features selections

-   How it affected the fitted model.

<!-- -->


*Its yet another paper on Malaria prediction in a community, this improves and focus on 
selections of features that best fit for Machine Learning models. Malaria among childhood in Sub-Saharan Africa is one of the leading cause of under-five mortality[@Maitlan], [@Camponova].

The interesting method of approach the author handles on elaborating LASSO regression for smallest number of predictors with the smallest prediction error. If this Machine Learning model is able to predict with smallest error, will help avoid adverse outcome after childhood malaria underscores the need for early detection and identification of high-risk populations.

The tricky part is Malaria can have wide range of covariates like physical, climatic and social factors, selecting minimal predictors which will have impact on outcome is where Lasso and Ridge regression will help.

## Data collection

The author has distributed data collection from rural and urban regions with the total of 2867 children under-five, using rapid diagnostic test(RDK) kit who got infected with malaria.

With the available dataset and literature search, they narrowed down to following variables


## Predictors

-   Child age, 
-   Number of under-five children in a household, 
-   Has mosquito bed net for sleeping, 
-   Sex of household head, 
-   Sex of a household member, 
-   Dwelling sprayed against mosquito last 12 months, 
-   Household wealth, 
-   Child-anaemia status, 
-   Has electricity in HH, 
-   Has a television in the household, 
-   Place of residence, 
-   The region of residence, 
-   Number of children who slept under mosquito bed net previous night, 
-   Insecticide-treated net available in the household, 
-   Number of household members.


## Alpha selection

For LASSO, author selected an alpha value of one, while for Ridge regression, an alpha value of zero was chosen. Since the alpha values for Elastic Net fall between zero and one 
(0<α<1), author used maximum likelihood estimation to determine the optimal alpha, which was estimated to be approximately 0.4187, based on 5-fold cross-validation repeated five times using the ‘caret’ package. Additionally, we estimated the minimum lambda (corresponding to the lowest mean squared error, or MSE) for LASSO, Ridge, and Elastic Net through maximum likelihood estimation under k-fold cross-validation.


### Modeling and Results

-   Lasso was able to predict using 11 features.

-   Ridge used all 15 features.

|Model |R-Square | RMSE(95% CI)|SD|AUC Value|Predictors|
|:------|:------ |:------------|:------|:------|:------|
|Lasso |0.196989 |0.9489 (0.9286, 0.9691) |0.0202 |81.20% |11 |
|Ridge |0.1972966 |1.0366 (1.0194, 1.0537) |0.0172 |81.20% |15 |

### Conclusion

-   Among two of logistic models using features picked by LASSO and Ridge methods. The model with LASSO features used 11 features and the Ridge model used 15 features. Both of these models explained about 20% of the differences in malaria rates among children. Each model had the same accuracy score for predictions, with an area under the curve (AUROC) of 81.2%. This means the models were quite good at predicting which children might have malaria.

-   Based on the principle of parsimony, the Lasso regression is preferred because it contains the smallest number of predictors and the smallest prediction error. LASSO model (RMSE = 0.9489, SD = 0.0202) where as Ridge (RMSE = 1.0366, SD = 0.0172) where RMSE is root mean square error i.e., prediction error

## Week 1 Paper 2

Prediction Modeling With Many Correlated and Zero-Inflated Predictors: Assessing the Non-negative Garrote Approach:[@wiley]

##### How do to handle zero-inflation data using 'ridge-garrote' methods?

-   To understand how to transform zero inflated predictor.

-   Particularly in correlated features

-   How it affected the fitted model.

<!-- -->


*This paper is of great inerest because it lists ways to handle zero values in the samples specifically in mass-spectrometry dataset.

The authors apply this technique and document there observations using predict kidney function using peptidomic features. The issue with mass-specrometry is that it produces intense data, sometime not all data is needed for accurate prediction another problem is that they are often subject to zero-inflation, means there is s spike at zero, which needs special transformation for linear models.


## Data collection

In Exploratory Data Analysis, Zero-inflated dataset is sometimes called as point mass values(PMVs, non-PMVs). There could be two reason, one natural sampleing data itself has zero or recording tool could be at fault. once eliminating the tool accuracy, we have to address actual PMVs.

Though there are methods of handling PVM, demonstrated by Gajjala et al.[@gajjala] used the least absolute shrinkage and selection operator (lasso), nonetheless, the optimal strategy for handling and analyzing zero-inflated and correlated predictors is uncertain, particularly in high-dimensional settings.


## Models
Author investigated commonly used practice, lasso and ridge regression, lasso provides selection and shrinkage of the regression coefficients for predictors based on L1-regularization where as ridge is better suited to handle correlated predictors.

In Lasso-ridge approach, author concludes, reduces the set of candidate predictors, while the second step retains the information from the zero-inflated predictors

In Ridge-lasso approach, author observes that there is a good blend of two-stage approach, which leads to explore ridge-garrote approach, first fits a ridge regression model then shrinkage factor for all predictors.

## Data selection

Motivated by real-life data from mass-spectrometry studies, author simulated data from a multivariate binomial and lognormal mixture distribution, 4 groups of 200 predictors with a group size of 50 predictors carefully introducing zeros in the dataset ranging 1/3 to 2/3 sample zeros.


### Modeling and Results


From each simulated dataset

-   Ridge, Lasso
-   Lasso-ridge
-   ridge-lasso
-   ridge-garrote regression were computed.

![Result](PVM.png){fig-alt="Result" width="400"}

Plot above displays the modeling methods across various cutoff levels for the maximum allowed proportion of PMVs (maxPMV) to explore how sensitive results are to these cutoff levels. Ridge, lasso-ridge and ridge-lasso mostly overlapped in terms of predictive accuracy in terms of RMSPE and R^2^
 

### Conclusion

-   Author concludes as ridge-garrote model was with the smallest number of selected predictors and demonstrated stability in predictor selection across increasing levels of maxPMV in contrast to lasso-ridge.
-   The lasso-ridge model, despite its predictive capabilities, showed similar variability in terms of the number of selected variables, can be seen by the sudden jump in selected predictors at a maximum percentage of zero-inflation of 90%.

## Week 2 Paper 1

Dealing with adverse drug reactions in the context of polypharmacy using regression models[@sen]


##### This study demonstrates that regression models with horseshoe or lasso priors are effective for analyzing ADRs.

-   Comprehensive consideration of multiple factors in large.

-   Working on sparse datasets

-   Which Improvs signal detection in polypharmacy.

<!-- -->


*Its common now a days adults taking multiple drugs(polypharmacy), which increases adverse drug reactions(ADRs) and rises signal which needs to be reported to FDA, Pharma companies needs data and right model to analyse there product against some common drugs to decrease ADR. Polypharmacy also includes other predictors like age, gender etc.

The interesting method of approach the author handles on elaborating horseshoe and LASSO regression on fewer potential overall positive predictors, which could make it suitable as a diagnostic tool with multiple factors in large, sparse datasets and improving signal detection in polypharmacy.

Falls or bleeding may serve as easily detectable ADRs outcome in older adults with polypharmacy. Though there are many generalized linear models, like logistic regression which can be interpreted, they lack analyzing many predictors with rare binary outcome this often may results in overfitting and poor predictive performances, which also results in under-reporting, so author explores more comprehensive analytically approach.

## Data collection

The author considered Adverse Drug Reaction in Emergency Departments (ADRED) dataset of 7175 records and considered 100 most frequently used drugs.


## Predictors
Author used Logistic regression models with horseshoe and lasso priors were used to analyze the association of drugs with the occurrence of the outcome ADRs. The most used 100 drugs included in the analysis served as possible predictors.



## Model selection

Author calculated the “global_scale” to be the “ratio of the expected number of non-zero coefficients to the expected number of zero coefficients, divided by the square root of the number of observations, essentially allowing a portion of the coefficients to escape the shrinkage. I might not understand why this was done, need to explore on this.


### Modeling and Results

Both regression were applied to the dataset to analyze associations between drugs and outcome ADRs in the context of polypharmacy. Some differences between the distribution of regression coefficients for drugs could be observed between the methods.

For Fall outcome, horseshoe model found age, female sex and inhibitors and substrate class drug showed association with outcome.

Similarly lasso regression highlighted a strong positive tendency for female sex and a positive tendency for the number of inhibitors and substrates taken.

For bleeding, both regression models, the confounding factor age showed a strong positive tendency, and the number of substrates and inhibitors a light positive tendency. As a difference, the lasso regression also exhibited a light positive tendency for the number of substrates.

### Conclusion

-   The horseshoe regression’s shrinkage was able to reduce the long list of drugs to a small group of candidates, which previously was linked to ADRs in the literature.

-   Similarly, lasso regression shrank most coefficients towards zero due to the Laplace distribution as well, but the shrinkage appeared to be less aggressive compared to the horseshoe model.

## Week 2 Paper 2

A comparison of penalised regression methods for informing the selection of predictive markers[@com]


##### study was to examine the predictive performance and feature (i.e., indicator) selection capability of common penalised logistic regression methods (LASSO, adaptive LASSO, and elastic-net), compared with traditional logistic regression and forward selection methods.

-   explore to identify which indicators maximize prediction when a large number of potential indicators is available

-   How the author tries to improving the accuracy and generalisability of predictive models

-   Or to balance accuracy and generalisability of prediction using Lasso, adaptive lasso and elastic-net

<!-- -->


*The author had two goals to explore, accuracy - ability of the model to correctly predict an outcome and generalisability - ability of the model to predict well given new data. I thought both are the need of the hour in the data science, so want to explore.


## Data collection

The author considered Australian Temperament Projects with multi-wave psychosocial development of young people from infancy to adulthood study dataset consisting of 1292 records.


## Predictors
Initially author chose 102 predictors in each models and young adult tobacco use outcome.they used mice pakage to create imputed dataset and continuous indicators were standardized by dividing scores by two times its SD.


## Model selection

Author used logistic, LASSO, adaptive LASSO and elastic-net regression methods. There were two penalties L1 - constraint based on the sum of the absolute value of regression coefficients and L2 - constraint based on the sum of the squared regression coefficients. 


### Modeling and Results

Author chose Area under curve (AUC) and harmonic mean of precision and recal (F1 score)  for predictive performance assessment. For the penalised logistic regression methods, robust indicators were considered as those which were selected in at least 80% of the cross-validation iterations also considered mean of the coefficients.

![Result](com.png){fig-alt="Result"}

Predictive performance: Box and whisker plot of AUC scores across 100 iterations of training and testing data splits. Dotted lines indicating median performance for logistic regression and forward selection

### Conclusion

-   The λ-min LASSO had the higher F1 score than the λ-1se model (Δ median F1 0.004), the λ-1se model outperformed the respective λ-min model for adaptive LASSO and elastic-net (Δ median F1 0.011–0.018).

-   There was notable similarity in the number of indicators selected in the LASSO and adaptive LASSO. The LASSO did, however, select slightly fewer indicators than the adaptive LASSO for both the λ-min (35 v 36) and λ-1se (5 v 7) models. Comparatively, both the LASSO and adaptive LASSO λ-min models contained several unique indicators (five and six, respectively)


## Week 3 Paper 1

#### Estimating U.S. housing price network connectedness: Evidence from dynamic Elastic Net, Lasso, and ridge vector autoregressive models[@USH]


##### The study is to see the effect of dynamic connectedness of random shokcks in housing price among the regions  using standard vector acutoregressive model(VAR) as well as three VAR models with shrinkage effect - Elastic Net, Lasso and Ridge..

-   Explore time series data in multivariate shrinkage using Lasso, Ridge models

-   How the author tries to take advantage of impulse response analysis and forecast error variance decomposition.

-   Finally demonstrate it on US housing market connectedness.

<!-- -->


*The author starts from Sims 'Macroeconomics and Reality' book reference to use Vector autoregressive model (AVR), this was interesting for me to know another type of model on time series data. Another term that is used in the paper is the 'spillovers of shocks', its defined as effects of a treatment extend to the control group, impacting their response despite not receiving the treatment directly.


## Data collection

The author find a limitation on the VAR approach, meaning that k = 51 (US States) and, hence, a joint estimation with a lag length of one would mean that $$k + k^2{p}=2652$$ parameters need to be estimated, but he only have T = 418 observations (2652 >> 418), which would mean that this model cannot be estimated. Using the equation-by-equation estimation procedure makes this feasible as it just estimates, k + 1 = 52 parameters (52<418). Additionally, as he deal with this high-dimensional network and employ different regularization methods that shrink and select parameters and compare the results with the standard equation-by-equation VAR model.


## Connectedness Measure
The starting point for the connectedness approach of Diebold and Yilmaz (2012)[@diebold] transforms the VAR(p) in (1) into its vector moving average representation using the Wold theorem. In the next step, the Generalized Forecast Error Variance Decomposition (GFEVD)5 of Koop, Pesaran, and Potter (1996)[@koop] and Pesaran and Shin (1998)[@shin], which is invariant to the ordering of the variables in the VAR. A rolling-window estimation with a 120-month (10-year) window and a 10-month forecast horizon, with repeated 10-fold cross-validation in each window is used on all 4 methods.


## Data and Model selection

Author used the seasonally adjusted monthly nominal house price data for the 50 states and the District of Columbia  from Freddie Mac, with the indices based on an ever-expanding database of loans purchased by either Freddie Mac or Fannie Mae.


### Modeling and Results

Its interesting to see the three methods that penalized parameter estimates using the Lasso, Ridge, and Net Elastic methods of penalization trended generally together, not differing by too much (below plot). The standard OLS results, however, produced higher total connectedness than the Lasso, Ridge, and Net Elastic methods during the beginning of the sample period 

![Result](ush.png){fig-alt="Result"}


### Conclusion

-   In using a VAR modeling approach that selects parameter estimates using Lasso, Ridge, and Elastic Net method, Lasso, Ridge, and Net Elastic methods of penalization trended generally together, not differing by too much. The standard OLS results, however, produced higher total connectedness than the other three methods.

## Week 3 Paper 2

## Estimating U.S. housing price network connectedness: Evidence from dynamic Elastic Net, Lasso, and ridge vector autoregressive models[@USH]


##### The study was to examine the predictive performance and feature (i.e., indicator) selection capability of common penalised logistic regression methods (LASSO, adaptive LASSO, and elastic-net), compared with traditional logistic regression and forward selection methods.

-   explore to identify which indicators maximize prediction when a large number of potential indicators is available

-   How the author tries to improving the accuracy and generalisability of predictive models

-   Or to balance accuracy and generalisability of prediction using Lasso, adaptive lasso and elastic-net

<!-- -->


*The author had two goals to explore, accuracy - ability of the model to correctly predict an outcome and generalisability - ability of the model to predict well given new data. I thought both are the need of the hour in the data science, so want to explore.


## Data collection

The author considered Australian Temperament Projects with multi-wave psychosocial development of young people from infancy to adulthood study dataset consisting of 1292 records.


## Predictors
Initially author chose 102 predictors in each models and young adult tobacco use outcome.they used mice pakage to create imputed dataset and continuous indicators were standardized by dividing scores by two times its SD.


## Model selection

Author used logistic, LASSO, adaptive LASSO and elastic-net regression methods. There were two penalties L1 - constraint based on the sum of the absolute value of regression coefficients and L2 - constraint based on the sum of the squared regression coefficients. 


### Modeling and Results

Author chose Area under curve (AUC) and harmonic mean of precision and recal (F1 score)  for predictive performance assessment. For the penalised logistic regression methods, robust indicators were considered as those which were selected in at least 80% of the cross-validation iterations also considered mean of the coefficients.

![Result](com.png){fig-alt="Result"}

Predictive performance: Box and whisker plot of AUC scores across 100 iterations of training and testing data splits. Dotted lines indicating median performance for logistic regression and forward selection

### Conclusion

-   The λ-min LASSO had the higher F1 score than the λ-1se model (Δ median F1 0.004), the λ-1se model outperformed the respective λ-min model for adaptive LASSO and elastic-net (Δ median F1 0.011–0.018).

-   There was notable similarity in the number of indicators selected in the LASSO and adaptive LASSO. The LASSO did, however, select slightly fewer indicators than the adaptive LASSO for both the λ-min (35 v 36) and λ-1se (5 v 7) models. Comparatively, both the LASSO and adaptive LASSO λ-min models contained several unique indicators (five and six, respectively)


## References
